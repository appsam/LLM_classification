{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af6247-a32f-4c09-8c63-92c346902d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ë°ì´í„° ì²˜ë¦¬ ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "\n",
    "# === ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë° ê²€ì¦ ë„êµ¬ ===\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# === ì´ìƒì¹˜ íƒì§€ ===\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# === íŠ¹ì§• ì¶”ì¶œ(Feature Engineering) ë„êµ¬ ===\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# === ë”¥ëŸ¬ë‹(ì„ë² ë”©, PPL) ëª¨ë¸ ===\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# === ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ===\n",
    "from joblib import Parallel, delayed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157047c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPU ì„¤ì • ë° ìµœì í™” ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# TF32 ìµœì í™” í™œì„±í™”\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"TF32 ìµœì í™” í™œì„±í™”ë¨\")\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    \"\"\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        max_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.2f} GB\")\n",
    "        print(f\"GPU ë©”ëª¨ë¦¬ ìºì‹œ: {reserved:.2f} GB\")\n",
    "        print(f\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {allocated/max_memory*100:.1f}%\")\n",
    "        print(f\"ì´ GPU ë©”ëª¨ë¦¬: {max_memory:.2f} GB\")\n",
    "\n",
    "# --- ë°ì´í„° ë¡œë“œ ---\n",
    "print(\"ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "train_df_original = pd.read_csv('data/train.csv', encoding='utf-8-sig')\n",
    "test_df = pd.read_csv('data/test.csv', encoding='utf-8-sig')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv', encoding='utf-8-sig')\n",
    "print(\"ë°ì´í„° ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "# # --- ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ (Mixed Precision ì ìš©) ---\n",
    "# print(\"ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "# bert_model = AutoModel.from_pretrained('klue/bert-base').half().to(device)  # FP16 ì ìš©\n",
    "# print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\")\n",
    "# print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_paragraph_df ì¬ìƒì„± (ì´ìƒíƒì§€ ë° ë ˆì´ë¸”ì— í•„ìš”)\n",
    "print(\"ğŸ“ í›ˆë ¨ ë°ì´í„° ë¬¸ë‹¨ ë¶„ë¦¬ ì¤‘...\")\n",
    "train_df_original['paragraphs'] = train_df_original['full_text'].str.split('\\n')\n",
    "train_paragraph_df = train_df_original.explode('paragraphs').rename(columns={'paragraphs': 'text'})\n",
    "train_paragraph_df = train_paragraph_df.dropna(subset=['text'])\n",
    "train_paragraph_df = train_paragraph_df[train_paragraph_df['text'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… ë¬¸ë‹¨ ë¶„ë¦¬ ì™„ë£Œ: {len(train_paragraph_df):,}ê°œ\")\n",
    "print(f\"ì‚¬ëŒ ê¸€: {(train_paragraph_df['generated'] == 0).sum():,}ê°œ\")\n",
    "print(f\"AI ê¸€: {(train_paragraph_df['generated'] == 1).sum():,}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53daad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ìµœì í™”ëœ íŠ¹ì§• ìƒì„± í•¨ìˆ˜ ---\n",
    "def get_bert_embeddings_optimized(texts, batch_size=512):\n",
    "    \"\"\"\n",
    "    A100 GPU ìµœì í™”ëœ BERT ì„ë² ë”© ìƒì„± í•¨ìˆ˜\n",
    "    - Mixed Precision (FP16) ì ìš©\n",
    "    - ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    - ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ì „ ì •ë¦¬\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}\")\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"BERT Embedding (ìµœì í™”ë¨)\"):\n",
    "        try:\n",
    "            batch = [str(t) for t in texts[i:i+batch_size]]\n",
    "\n",
    "            # í† í¬ë‚˜ì´ì§•\n",
    "            batch_dict = bert_tokenizer(\n",
    "                batch,\n",
    "                max_length=512,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "\n",
    "            # Mixed Precisionìœ¼ë¡œ Forward Pass\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = bert_model(**batch_dict)\n",
    "\n",
    "            # ì„ë² ë”© ì¶”ì¶œ (float32ë¡œ ë³€í™˜í•˜ì—¬ ì •í™•ë„ ìœ ì§€)\n",
    "            embeddings = outputs.pooler_output.float()\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "            # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ\n",
    "            del batch_dict, outputs, embeddings\n",
    "\n",
    "            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (10ë°°ì¹˜ë§ˆë‹¤)\n",
    "            if i % (10 * batch_size) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"\\në©”ëª¨ë¦¬ ë¶€ì¡± ë°œìƒ! ë°°ì¹˜ í¬ê¸°ë¥¼ {batch_size//2}ë¡œ ì¤„ì…ë‹ˆë‹¤.\")\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                return get_bert_embeddings_optimized(texts, batch_size//2)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def find_optimal_batch_size(sample_texts, start_batch=512, max_batch=2048):\n",
    "    \"\"\"ìµœì  ë°°ì¹˜ í¬ê¸° ìë™ íƒìƒ‰\"\"\"\n",
    "    print(\"ìµœì  ë°°ì¹˜ í¬ê¸°ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "    for batch_size in [start_batch, start_batch*2, max_batch]:\n",
    "        try:\n",
    "            print(f\"ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "            # ì†ŒëŸ‰ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸\n",
    "            test_sample = sample_texts[:min(batch_size*2, len(sample_texts))]\n",
    "            _ = get_bert_embeddings_optimized(test_sample, batch_size=batch_size)\n",
    "            print(f\"ë°°ì¹˜ í¬ê¸° {batch_size} ì„±ê³µ!\")\n",
    "            optimal_batch = batch_size\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"ë°°ì¹˜ í¬ê¸° {batch_size} ë©”ëª¨ë¦¬ ë¶€ì¡±\")\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return optimal_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eafe49",
   "metadata": {},
   "source": [
    "# í›ˆë ¨ ë°ì´í„° ì„ë² ë”© + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d30da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 2. ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸° ---\n",
    "# sample_texts = train_paragraph_df['text'].head(1000).tolist()\n",
    "# optimal_batch_size = find_optimal_batch_size(sample_texts)\n",
    "# print(f\"ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}\")\n",
    "\n",
    "# # --- 3. í›ˆë ¨ ë°ì´í„° ëª¨ë“  ë¬¸ë‹¨ì— ëŒ€í•œ íŠ¹ì§• ìƒì„± (ìµœì í™”ë¨) ---\n",
    "# print(f\"\\n[í›ˆë ¨ ë¬¸ë‹¨ ë°ì´í„°] íŠ¹ì§• ìƒì„± ì‹œì‘... (ë°°ì¹˜ í¬ê¸°: {optimal_batch_size})\")\n",
    "\n",
    "# # 3-1. BERT ì„ë² ë”© (ì œëª©)\n",
    "# print(\"ì œëª© BERT ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "# print_gpu_utilization()\n",
    "# title_embeddings = get_bert_embeddings_optimized(\n",
    "#     train_paragraph_df['title'].tolist(),\n",
    "#     batch_size=optimal_batch_size\n",
    "# )\n",
    "# print(\"ì œëª© ì„ë² ë”© ì™„ë£Œ!\")\n",
    "# print_gpu_utilization()\n",
    "\n",
    "# # 3-2. BERT ì„ë² ë”© (ë³¸ë¬¸)\n",
    "# print(\"ë³¸ë¬¸ BERT ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "# text_embeddings = get_bert_embeddings_optimized(\n",
    "#     train_paragraph_df['text'].tolist(),\n",
    "#     batch_size=optimal_batch_size\n",
    "# )\n",
    "# print(\"ë³¸ë¬¸ ì„ë² ë”© ì™„ë£Œ!\")\n",
    "# print_gpu_utilization()\n",
    "\n",
    "# # 3-3. BERT ì„ë² ë”© ê²°í•©\n",
    "# X_train_p_emb = np.concatenate([title_embeddings, text_embeddings], axis=1)\n",
    "# print(f\"ê²°í•©ëœ BERT ì„ë² ë”© shape: {X_train_p_emb.shape}\")\n",
    "\n",
    "# # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "# del title_embeddings, text_embeddings\n",
    "# gc.collect()\n",
    "\n",
    "# print(\"TF-IDF + SVD íŠ¹ì§• ìƒì„± ì¤‘...\")\n",
    "# tfidf_pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=10000)),\n",
    "#     ('svd', TruncatedSVD(n_components=128, random_state=42))\n",
    "# ])\n",
    "# X_train_p_tfidf = tfidf_pipeline.fit_transform(train_paragraph_df['text'])\n",
    "# print(f\"TF-IDF + SVD shape: {X_train_p_tfidf.shape}\")\n",
    "\n",
    "# # 3-5. ëª¨ë“  íŠ¹ì§• ê²°í•©\n",
    "# print(\"ëª¨ë“  íŠ¹ì§•ì„ ê²°í•©í•©ë‹ˆë‹¤...\")\n",
    "# X_train_paragraph_features = np.concatenate([X_train_p_emb, X_train_p_tfidf], axis=1)\n",
    "# print(f\"ìµœì¢… íŠ¹ì§• í–‰ë ¬ shape: {X_train_paragraph_features.shape}\")\n",
    "\n",
    "# # --- 4. ê²°ê³¼ ì €ì¥ ---\n",
    "# print(\"íŠ¹ì§• í–‰ë ¬ì„ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "# np.save('X_train_paragraph_features.npy', X_train_paragraph_features)\n",
    "# print(\"ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "# # --- 5. ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ ---\n",
    "# print(\"\\n=== ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ ===\")\n",
    "# print_gpu_utilization()\n",
    "\n",
    "# # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "# print(\"ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì»¬ëŸ¼ëª… í™•ì¸ ë° ë³€í™˜ (í•„ìš”ì‹œ)\n",
    "# if 'paragraph_text' in test_df.columns:\n",
    "#     test_df = test_df.rename(columns={'paragraph_text': 'text'})\n",
    "#     print(\"ì»¬ëŸ¼ëª… ë³€í™˜: paragraph_text â†’ text\")\n",
    "\n",
    "# print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {test_df.shape}\")\n",
    "# print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼: {list(test_df.columns)}\")\n",
    "\n",
    "# # --- 2. ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸° (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ì¤€) ---\n",
    "# sample_test_texts = test_df['text'].head(1000).tolist()\n",
    "# optimal_batch_size = find_optimal_batch_size(sample_test_texts)\n",
    "# print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}\")\n",
    "\n",
    "# # --- 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì§• ìƒì„± (ìµœì í™”ë¨) ---\n",
    "# print(f\"\\n[í…ŒìŠ¤íŠ¸ ë°ì´í„°] íŠ¹ì§• ìƒì„± ì‹œì‘... (ë°°ì¹˜ í¬ê¸°: {optimal_batch_size})\")\n",
    "\n",
    "# # 3-1. BERT ì„ë² ë”© (ì œëª©)\n",
    "# print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì œëª© BERT ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "# print_gpu_utilization()\n",
    "# test_title_embeddings = get_bert_embeddings_optimized(\n",
    "#     test_df['title'].tolist(),\n",
    "#     batch_size=optimal_batch_size\n",
    "# )\n",
    "# print(\"í…ŒìŠ¤íŠ¸ ì œëª© ì„ë² ë”© ì™„ë£Œ!\")\n",
    "# print_gpu_utilization()\n",
    "\n",
    "# # 3-2. BERT ì„ë² ë”© (ë³¸ë¬¸)\n",
    "# print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³¸ë¬¸ BERT ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "# test_text_embeddings = get_bert_embeddings_optimized(\n",
    "#     test_df['text'].tolist(),\n",
    "#     batch_size=optimal_batch_size\n",
    "# )\n",
    "# print(\"í…ŒìŠ¤íŠ¸ ë³¸ë¬¸ ì„ë² ë”© ì™„ë£Œ!\")\n",
    "# print_gpu_utilization()\n",
    "\n",
    "# # 3-3. BERT ì„ë² ë”© ê²°í•©\n",
    "# X_test_p_emb = np.concatenate([test_title_embeddings, test_text_embeddings], axis=1)\n",
    "# print(f\"í…ŒìŠ¤íŠ¸ ê²°í•©ëœ BERT ì„ë² ë”© shape: {X_test_p_emb.shape}\")\n",
    "\n",
    "# # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "# del test_title_embeddings, test_text_embeddings\n",
    "# gc.collect()\n",
    "\n",
    "# # --- 3-4. TF-IDF + SVD ì²˜ë¦¬ ---\n",
    "# print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— TF-IDF transform ì ìš©...\")\n",
    "# X_test_p_tfidf = tfidf_pipeline.transform(test_df['text'])\n",
    "# print(f\"í…ŒìŠ¤íŠ¸ TF-IDF + SVD shape: {X_test_p_tfidf.shape}\")\n",
    "\n",
    "# # --- 3-5. ëª¨ë“  íŠ¹ì§• ê²°í•© ---\n",
    "# print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ë“  íŠ¹ì§•ì„ ê²°í•©í•©ë‹ˆë‹¤...\")\n",
    "# X_test_paragraph_features = np.concatenate([X_test_p_emb, X_test_p_tfidf], axis=1)\n",
    "# print(f\"í…ŒìŠ¤íŠ¸ ìµœì¢… íŠ¹ì§• í–‰ë ¬ shape: {X_test_paragraph_features.shape}\")\n",
    "\n",
    "# # --- 4. ê²°ê³¼ ì €ì¥ ---\n",
    "# print(\"\\ní…ŒìŠ¤íŠ¸ íŠ¹ì§• í–‰ë ¬ì„ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
    "# np.save('X_test_paragraph_features.npy', X_test_paragraph_features)\n",
    "# print(\"âœ… X_test_paragraph_features.npy ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "load-train-npy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í›ˆë ¨ íŠ¹ì§• ë¡œë“œ ì™„ë£Œ: (1226364, 1664)\n"
     ]
    }
   ],
   "source": [
    "# --- ë¯¸ë¦¬ ìƒì„±ëœ íŠ¹ì§•(.npy) íŒŒì¼ ë¡œë“œ ---\n",
    "print(\"ğŸ’¾ ë¯¸ë¦¬ ìƒì„±ëœ í›ˆë ¨ ë°ì´í„° íŠ¹ì§• íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# data í´ë”ì— ì €ì¥ëœ .npy íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "X_train_paragraph_features = np.load('data/X_train_paragraph_features.npy')\n",
    "\n",
    "print(f\"âœ… í›ˆë ¨ íŠ¹ì§• ë¡œë“œ ì™„ë£Œ: {X_train_paragraph_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8fe0882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒíƒì§€ ì‹œì‘\n",
      "ì‚¬ëŒ ê¸€ ë¬¸ë‹¨: 1,125,652ê°œ\n",
      "  1ë‹¨ê³„: Isolation Forest...\n",
      "  1ì°¨ í•„í„°ë§: 956,804ê°œ ë‚¨ìŒ\n",
      "  2ë‹¨ê³„: LOF...\n",
      "âœ… í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒíƒì§€ ì™„ë£Œ! (12734.1ì´ˆ)\n",
      "  ì œê±°ëœ ë…¸ì´ì¦ˆ: 216,689ê°œ (19.3%)\n",
      "  ì •ì œëœ ì‚¬ëŒ ê¸€: 908,963ê°œ\n",
      "\n",
      "ğŸ“Š ì •ì œëœ í›ˆë ¨ ë°ì´í„°: (1009675, 1664)\n",
      "  ì •ì œëœ ì‚¬ëŒ ê¸€: 908,963ê°œ\n",
      "  AI ê¸€: 100,712ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒíƒì§€ ì‹œì‘\")\n",
    "start_time = time.time()\n",
    "\n",
    "# ì‚¬ëŒ ê¸€ ì¸ë±ìŠ¤ ì¶”ì¶œ (ë¼ë²¨ 0)\n",
    "human_indices = np.where(train_paragraph_df['generated'] == 0)[0]\n",
    "print(f\"ì‚¬ëŒ ê¸€ ë¬¸ë‹¨: {len(human_indices):,}ê°œ\")\n",
    "\n",
    "# 1ë‹¨ê³„: Isolation Forest (ë¹ ë¥¸ 1ì°¨ í•„í„°ë§)\n",
    "print(\"  1ë‹¨ê³„: Isolation Forest...\")\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.15,       # 15% ì´ìƒì¹˜ ê°€ì •\n",
    "    n_estimators=200,\n",
    "    max_samples='auto',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "iso_outliers = iso_forest.fit_predict(X_train_paragraph_features[human_indices])\n",
    "stage1_clean = human_indices[iso_outliers == 1]\n",
    "print(f\"  1ì°¨ í•„í„°ë§: {len(stage1_clean):,}ê°œ ë‚¨ìŒ\")\n",
    "\n",
    "# 2ë‹¨ê³„: LOF (ì •êµí•œ 2ì°¨ í•„í„°ë§)\n",
    "print(\"  2ë‹¨ê³„: LOF...\")\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=min(50, len(stage1_clean) // 20),\n",
    "    contamination=0.05,       # 5% ì¶”ê°€ ì œê±°\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lof_outliers = lof.fit_predict(X_train_paragraph_features[stage1_clean])\n",
    "final_clean_indices = stage1_clean[lof_outliers == 1]\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "removed = len(human_indices) - len(final_clean_indices)\n",
    "\n",
    "print(f\"âœ… í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒíƒì§€ ì™„ë£Œ! ({elapsed:.1f}ì´ˆ)\")\n",
    "print(f\"  ì œê±°ëœ ë…¸ì´ì¦ˆ: {removed:,}ê°œ ({removed/len(human_indices)*100:.1f}%)\")\n",
    "print(f\"  ì •ì œëœ ì‚¬ëŒ ê¸€: {len(final_clean_indices):,}ê°œ\")\n",
    "\n",
    "# ì •ì œëœ í›ˆë ¨ ë°ì´í„° êµ¬ì„±\n",
    "# ì •ì œëœ ì‚¬ëŒ ê¸€ + ëª¨ë“  AI ê¸€\n",
    "ai_indices = np.where(train_paragraph_df['generated'] == 1)[0]\n",
    "clean_indices = np.concatenate([final_clean_indices, ai_indices])\n",
    "\n",
    "X_train_clean = X_train_paragraph_features[clean_indices]\n",
    "y_train_clean = train_paragraph_df['generated'].iloc[clean_indices].values\n",
    "\n",
    "print(f\"\\nğŸ“Š ì •ì œëœ í›ˆë ¨ ë°ì´í„°: {X_train_clean.shape}\")\n",
    "print(f\"  ì •ì œëœ ì‚¬ëŒ ê¸€: {(y_train_clean == 0).sum():,}ê°œ\")\n",
    "print(f\"  AI ê¸€: {(y_train_clean == 1).sum():,}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9917074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_text</th>\n",
       "      <th>generated</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬</td>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬</td>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>ë§ˆìš°ì´ì„¬ì—ì„œ ë‚¨ì„œìª½ìœ¼ë¡œ ì•½ 11km ì •ë„ ë–¨ì–´ì§„ ê³³ì— ìœ„ì¹˜í•˜ë©° ë¼ë‚˜ì´ì„¬ì˜ ë‚¨ë™ìª½ì—...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬</td>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>1000ë…„ê²½ë¶€í„° ì‚¬ëŒì´ ê±°ì£¼í–ˆìœ¼ë©° í•´ì•ˆ ì§€ëŒ€ì—ëŠ” ì†Œê·œëª¨ ì„ì‹œ ì–´ì´Œì´ í˜•ì„±ë˜ì—ˆë‹¤. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬</td>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>1830ë…„ëŒ€ì—ëŠ” í•˜ì™€ì´ ì™•êµ­ì˜ ì¹´ë©”í•˜ë©”í•˜ 3ì„¸ êµ­ì™•ì— ì˜í•´ ë‚¨ì ì£„ìˆ˜ë“¤ì˜ ìœ í˜•ì§€ë¡œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬</td>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>1910ë…„ë¶€í„° 1918ë…„ê¹Œì§€ í•˜ì™€ì´ ì¤€ì£¼ê°€ ì„¬ì˜ ì›ë˜ ëª¨ìŠµì„ ë³µì›í•˜ê¸° ìœ„í•´ ì´ ì„¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬</td>\n",
       "      <td>ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>1941ë…„ 12ì›” 7ì¼ì— ì¼ì–´ë‚œ ì¼ë³¸ ì œêµ­ í•´êµ°ì˜ ì§„ì£¼ë§Œ ê³µê²©ì„ ê³„ê¸°ë¡œ ì¹´í˜¸ì˜¬ë¼ì›¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ìš©ì–´ëŠ” ê°ì ë‹¤ë¥¸ ì§„í™” ë‹¨ê³„ì— ìˆëŠ” ì—¬ëŸ¬ ê°€ì§€ ë³„ì— ì ìš©ë˜ëŠ”ë°, ì´ë“¤ ëª¨ë‘ ì£¼ê³„ì—´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±ì´ë¼ëŠ” ëª…ì¹­ì€ ì¢…ì¢… ë§¤ìš° í¬ê³  ëœ¨ê±°ìš´ ì£¼ê³„ì—´ì„±ê³¼ ê°™ì´, ë‹¤ë¥¸ ë¬´ê²ê³  ë°ì€ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±ì€ ì—„ê²©íˆ ì •ì˜ëœ ë‹¨ì–´ê°€ ì•„ë‹ˆì–´ì„œ ì„œë¡œ ë‹¤ë¥¸ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë³„ì— í­ë„“ê²Œ ì‚¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ë§¤ìš° ì°¨ê°‘ê³  ì–´ë‘ìš´ ë³„ì€, ì ìƒ‰ê±°ì„± ë‹¨ê³„ë¥¼ ì§€ë‚œ ì¤‘ê°„ ì§ˆëŸ‰ì˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ê±°ì„±ì— ëŒ€í•´ì„œ ëª…í™•í•œ ìµœëŒ€ í•œê³„ëŠ” ì—†ì§€ë§Œ, Oí˜• ì´ˆë°˜ì˜ ì£¼ê³„ì—´ì„±ê³¼ ê·¸ì™€ ê±°ì˜ ë™ì¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>HR ë„í‘œì˜ ì²­ìƒ‰ê±°ì„± ì˜ì—­ì—ì„œ ë°œê²¬ë˜ëŠ” ë³„ì€ ì¼ìƒì—ì„œì˜ ë‹¨ê³„ê°€ ê°ì í¬ê²Œ ë‹¤ë¥¼ ìˆ˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ê°€ì¥ ê°„ë‹¨í•œ ê²½ìš°ì—ì„œ ëœ¨ê²ê³  ë°ì€ ë³„ì€ ì¤‘ì‹¬í•µì˜ ìˆ˜ì†Œê°€ ê³ ê°ˆë  ë•Œ íŒ½ì°½í•˜ê¸° ì‹œì‘í•˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>BHB ë³„ì€ ì•„ì§ ì»¤ë‹¤ë€ ìˆ˜ì†Œê»ì§ˆì„ ê°€ì§€ê³  ìˆë‹¤ í•´ë„ ë” ì§„í™”í•˜ì—¬ í—¬ë¥¨ ì—°ì†Œí•µì„ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ë³´í†µ ì²­ìƒ‰ê±°ì„±ìœ¼ë¡œ ë¶ˆë¦¬ì§€ ì•ŠëŠ” ë” ì§„í™”í•œ ëœ¨ê±°ìš´ ë³„ë“¤ë„ ìˆëŠ”ë°, ë§¤ìš° ë°ê³  ê·¹ë‹¨ì ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ì²­ìƒ‰ê±°ì„±</td>\n",
       "      <td>ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...</td>\n",
       "      <td>0</td>\n",
       "      <td>ìˆœì „íˆ ì´ë¡ ì ì¸ ì˜ì—­ì˜ ë³„ì€ ì ìƒ‰ì™œì„±ì´ ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì¡° ë…„ì´ ì§€ë‚œ ë¯¸ë˜ì— ì¤‘ì‹¬í•µì˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­</td>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­</td>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² ì¶œì‹  ë³‘ì‚¬ë“¤ì€ 1918ë…„ ì´ˆë¶€í„° ì´ë¯¸ ë¶ˆì˜¨í•œ ë‚Œìƒˆë¥¼ ë³´ì´ê¸° ì‹œì‘í•˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­</td>\n",
       "      <td>ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1918ë…„ 10ì›”, ë…ì¼ í™©ë¦½í•´êµ°ì˜ ìˆ˜ë³‘ë“¤ì´ ì˜êµ­ ì™•ë¦½í•´êµ°ê³¼ ì‹¸ìš°ê¸° ìœ„í•´ ì¶œí•­í•˜ë¼...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title                                          full_text  \\\n",
       "0             ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬  ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...   \n",
       "1             ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬  ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...   \n",
       "2             ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬  ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...   \n",
       "3             ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬  ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...   \n",
       "4             ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬  ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...   \n",
       "5             ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬  ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...   \n",
       "6               ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "7               ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "8               ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "9               ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "10              ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "11              ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "12              ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "13              ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "14              ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "15              ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "16              ì²­ìƒ‰ê±°ì„±  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...   \n",
       "17  ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­  ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...   \n",
       "18  ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­  ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...   \n",
       "19  ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­  ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...   \n",
       "\n",
       "    generated                                               text  \n",
       "0           0  ì¹´í˜¸ì˜¬ë¼ì›¨ì„¬ì€ í•˜ì™€ì´ ì œë„ë¥¼ êµ¬ì„±í•˜ëŠ” 8ê°œì˜ í™”ì‚°ì„¬ ê°€ìš´ë° í•˜ë‚˜ë¡œ ë©´ì ì€ 115.5...  \n",
       "1           0   ë§ˆìš°ì´ì„¬ì—ì„œ ë‚¨ì„œìª½ìœ¼ë¡œ ì•½ 11km ì •ë„ ë–¨ì–´ì§„ ê³³ì— ìœ„ì¹˜í•˜ë©° ë¼ë‚˜ì´ì„¬ì˜ ë‚¨ë™ìª½ì—...  \n",
       "2           0   1000ë…„ê²½ë¶€í„° ì‚¬ëŒì´ ê±°ì£¼í–ˆìœ¼ë©° í•´ì•ˆ ì§€ëŒ€ì—ëŠ” ì†Œê·œëª¨ ì„ì‹œ ì–´ì´Œì´ í˜•ì„±ë˜ì—ˆë‹¤. ...  \n",
       "3           0   1830ë…„ëŒ€ì—ëŠ” í•˜ì™€ì´ ì™•êµ­ì˜ ì¹´ë©”í•˜ë©”í•˜ 3ì„¸ êµ­ì™•ì— ì˜í•´ ë‚¨ì ì£„ìˆ˜ë“¤ì˜ ìœ í˜•ì§€ë¡œ...  \n",
       "4           0   1910ë…„ë¶€í„° 1918ë…„ê¹Œì§€ í•˜ì™€ì´ ì¤€ì£¼ê°€ ì„¬ì˜ ì›ë˜ ëª¨ìŠµì„ ë³µì›í•˜ê¸° ìœ„í•´ ì´ ì„¬...  \n",
       "5           0   1941ë…„ 12ì›” 7ì¼ì— ì¼ì–´ë‚œ ì¼ë³¸ ì œêµ­ í•´êµ°ì˜ ì§„ì£¼ë§Œ ê³µê²©ì„ ê³„ê¸°ë¡œ ì¹´í˜¸ì˜¬ë¼ì›¨...  \n",
       "6           0  ì²œë¬¸í•™ì—ì„œ ì²­ìƒ‰ê±°ì„±(é‘è‰²å·¨æ˜Ÿ, )ì€ ê´‘ë„ ë¶„ë¥˜ì—ì„œ IIIí˜•(ê±°ì„±) ë˜ëŠ” IIí˜•(ë°ì€...  \n",
       "7           0   ìš©ì–´ëŠ” ê°ì ë‹¤ë¥¸ ì§„í™” ë‹¨ê³„ì— ìˆëŠ” ì—¬ëŸ¬ ê°€ì§€ ë³„ì— ì ìš©ë˜ëŠ”ë°, ì´ë“¤ ëª¨ë‘ ì£¼ê³„ì—´...  \n",
       "8           0   ì²­ìƒ‰ê±°ì„±ì´ë¼ëŠ” ëª…ì¹­ì€ ì¢…ì¢… ë§¤ìš° í¬ê³  ëœ¨ê±°ìš´ ì£¼ê³„ì—´ì„±ê³¼ ê°™ì´, ë‹¤ë¥¸ ë¬´ê²ê³  ë°ì€ ...  \n",
       "9           0   ì²­ìƒ‰ê±°ì„±ì€ ì—„ê²©íˆ ì •ì˜ëœ ë‹¨ì–´ê°€ ì•„ë‹ˆì–´ì„œ ì„œë¡œ ë‹¤ë¥¸ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë³„ì— í­ë„“ê²Œ ì‚¬...  \n",
       "10          0   ì²­ìƒ‰ê±°ì„±ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ë§¤ìš° ì°¨ê°‘ê³  ì–´ë‘ìš´ ë³„ì€, ì ìƒ‰ê±°ì„± ë‹¨ê³„ë¥¼ ì§€ë‚œ ì¤‘ê°„ ì§ˆëŸ‰ì˜...  \n",
       "11          0   ê±°ì„±ì— ëŒ€í•´ì„œ ëª…í™•í•œ ìµœëŒ€ í•œê³„ëŠ” ì—†ì§€ë§Œ, Oí˜• ì´ˆë°˜ì˜ ì£¼ê³„ì—´ì„±ê³¼ ê·¸ì™€ ê±°ì˜ ë™ì¼...  \n",
       "12          0   HR ë„í‘œì˜ ì²­ìƒ‰ê±°ì„± ì˜ì—­ì—ì„œ ë°œê²¬ë˜ëŠ” ë³„ì€ ì¼ìƒì—ì„œì˜ ë‹¨ê³„ê°€ ê°ì í¬ê²Œ ë‹¤ë¥¼ ìˆ˜...  \n",
       "13          0   ê°€ì¥ ê°„ë‹¨í•œ ê²½ìš°ì—ì„œ ëœ¨ê²ê³  ë°ì€ ë³„ì€ ì¤‘ì‹¬í•µì˜ ìˆ˜ì†Œê°€ ê³ ê°ˆë  ë•Œ íŒ½ì°½í•˜ê¸° ì‹œì‘í•˜...  \n",
       "14          0   BHB ë³„ì€ ì•„ì§ ì»¤ë‹¤ë€ ìˆ˜ì†Œê»ì§ˆì„ ê°€ì§€ê³  ìˆë‹¤ í•´ë„ ë” ì§„í™”í•˜ì—¬ í—¬ë¥¨ ì—°ì†Œí•µì„ ...  \n",
       "15          0   ë³´í†µ ì²­ìƒ‰ê±°ì„±ìœ¼ë¡œ ë¶ˆë¦¬ì§€ ì•ŠëŠ” ë” ì§„í™”í•œ ëœ¨ê±°ìš´ ë³„ë“¤ë„ ìˆëŠ”ë°, ë§¤ìš° ë°ê³  ê·¹ë‹¨ì ...  \n",
       "16          0   ìˆœì „íˆ ì´ë¡ ì ì¸ ì˜ì—­ì˜ ë³„ì€ ì ìƒ‰ì™œì„±ì´ ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì¡° ë…„ì´ ì§€ë‚œ ë¯¸ë˜ì— ì¤‘ì‹¬í•µì˜...  \n",
       "17          0  ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² í‰ì˜íšŒ ê³µí™”êµ­ì€ 1ì°¨ëŒ€ì „ ë§ê¸° ë…ì¼ í˜ëª… ì™€ì¤‘ì— ì—˜ììŠ¤-ë¡œíŠ¸ë§ê²ì—ì„œ...  \n",
       "18          0   ì—˜ììŠ¤-ë¡œíŠ¸ë§ê² ì¶œì‹  ë³‘ì‚¬ë“¤ì€ 1918ë…„ ì´ˆë¶€í„° ì´ë¯¸ ë¶ˆì˜¨í•œ ë‚Œìƒˆë¥¼ ë³´ì´ê¸° ì‹œì‘í•˜...  \n",
       "19          0   1918ë…„ 10ì›”, ë…ì¼ í™©ë¦½í•´êµ°ì˜ ìˆ˜ë³‘ë“¤ì´ ì˜êµ­ ì™•ë¦½í•´êµ°ê³¼ ì‹¸ìš°ê¸° ìœ„í•´ ì¶œí•­í•˜ë¼...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paragraph_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba12f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ StratifiedGroupKFold LightGBM í•™ìŠµ ì‹œì‘ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
      "ê·¸ë£¹ êµì°¨ê²€ì¦ì„ ìœ„í•œ ê·¸ë£¹ ìƒì„± ì™„ë£Œ: 92861ê°œ ê³ ìœ  ê¸€\n",
      "\n",
      "ğŸ”„ 5-Fold Group êµì°¨ê²€ì¦ ì‹œì‘...\n",
      "\n",
      "ğŸ“ Fold 1/5\n",
      "  í›ˆë ¨ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.100\n",
      "  ê²€ì¦ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.099\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.712611\n",
      "  Fold 1 AUC: 0.71261\n",
      "\n",
      "ğŸ“ Fold 2/5\n",
      "  í›ˆë ¨ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.100\n",
      "  ê²€ì¦ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.100\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[159]\tvalid's auc: 0.709926\n",
      "  Fold 2 AUC: 0.70993\n",
      "\n",
      "ğŸ“ Fold 3/5\n",
      "  í›ˆë ¨ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.100\n",
      "  ê²€ì¦ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.100\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid's auc: 0.72295\n",
      "  Fold 3 AUC: 0.72295\n",
      "\n",
      "ğŸ“ Fold 4/5\n",
      "  í›ˆë ¨ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.101\n",
      "  ê²€ì¦ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.096\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's auc: 0.712711\n",
      "  Fold 4 AUC: 0.71271\n",
      "\n",
      "ğŸ“ Fold 5/5\n",
      "  í›ˆë ¨ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.099\n",
      "  ê²€ì¦ ì„¸íŠ¸ AI ë¹„ìœ¨: 0.104\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[153]\tvalid's auc: 0.707323\n",
      "  Fold 5 AUC: 0.70732\n",
      "\n",
      "ğŸ† êµì°¨ê²€ì¦ ê²°ê³¼ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¨):\n",
      "  í‰ê·  AUC: 0.71310 (Â±0.00531)\n",
      "  OOF AUC: 0.71047\n",
      "  ìµœê³  Fold AUC: 0.72295\n",
      "  ìµœì € Fold AUC: 0.70732\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¯ StratifiedGroupKFold LightGBM í•™ìŠµ ì‹œì‘ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\")\n",
    "\n",
    "# Optuna ìµœì í™”ëœ íŒŒë¼ë¯¸í„°\n",
    "best_params = {\n",
    "    'learning_rate': 0.043,\n",
    "    'num_leaves': 41,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.84,\n",
    "    'lambda_l1': 9.28e-06,\n",
    "    'lambda_l2': 0.0021,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 1000,\n",
    "    'device': 'cpu',  # GPU â†’ CPU ë³€ê²½\n",
    "    'n_jobs': 16\n",
    "}\n",
    "\n",
    "# --- ê·¸ë£¹ êµì°¨ê²€ì¦ ì„¤ì • ---\n",
    "# 1. ê·¸ë£¹ ì •ë³´ ìƒì„± (ì›ë³¸ ê¸€ ID ê¸°ì¤€)\n",
    "# clean_indicesëŠ” ì •ì œëœ ë¬¸ë‹¨ë“¤ì˜ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.\n",
    "# ì´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ train_paragraph_dfì—ì„œ ê¸€ ID('id')ë¥¼ ê°€ì ¸ì™€ ê·¸ë£¹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "groups = train_paragraph_df['title'].iloc[clean_indices].values\n",
    "print(f\"ê·¸ë£¹ êµì°¨ê²€ì¦ì„ ìœ„í•œ ê·¸ë£¹ ìƒì„± ì™„ë£Œ: {len(np.unique(groups))}ê°œ ê³ ìœ  ê¸€\")\n",
    "\n",
    "# 2. StratifiedGroupKFold ì„¤ì •\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# OOF ì˜ˆì¸¡ì„ ìœ„í•œ ë°°ì—´ ì´ˆê¸°í™”\n",
    "oof_predictions = np.zeros(len(X_train_clean))\n",
    "fold_models = []\n",
    "fold_scores = []\n",
    "\n",
    "print(f\"\\nğŸ”„ 5-Fold Group êµì°¨ê²€ì¦ ì‹œì‘...\")\n",
    "\n",
    "# 3. êµì°¨ê²€ì¦ ë£¨í”„ ë³€ê²½\n",
    "# split ë©”ì„œë“œì— groups ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_clean, y_train_clean, groups)):\n",
    "    print(f\"\\nğŸ“ Fold {fold + 1}/5\")\n",
    "    \n",
    "    X_fold_train, X_fold_val = X_train_clean[train_idx], X_train_clean[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train_clean[train_idx], y_train_clean[val_idx]\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "    train_ratio = y_fold_train.mean()\n",
    "    val_ratio = y_fold_val.mean()\n",
    "    print(f\"  í›ˆë ¨ ì„¸íŠ¸ AI ë¹„ìœ¨: {train_ratio:.3f}\")\n",
    "    print(f\"  ê²€ì¦ ì„¸íŠ¸ AI ë¹„ìœ¨: {val_ratio:.3f}\")\n",
    "    \n",
    "    # LightGBM ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_dataset = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
    "    val_dataset = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_dataset)\n",
    "    \n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    fold_model = lgb.train(\n",
    "        best_params,\n",
    "        train_dataset,\n",
    "        valid_sets=[val_dataset],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=0)  # ë¡œê·¸ ì¶œë ¥ ìµœì†Œí™”\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡\n",
    "    val_pred = fold_model.predict(X_fold_val, num_iteration=fold_model.best_iteration)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # AUC ì ìˆ˜ ê³„ì‚°\n",
    "    fold_auc = roc_auc_score(y_fold_val, val_pred)\n",
    "    fold_scores.append(fold_auc)\n",
    "    fold_models.append(fold_model)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1} AUC: {fold_auc:.5f}\")\n",
    "\n",
    "# ì „ì²´ OOF AUC ê³„ì‚°\n",
    "oof_auc = roc_auc_score(y_train_clean, oof_predictions)\n",
    "mean_auc = np.mean(fold_scores)\n",
    "std_auc = np.std(fold_scores)\n",
    "\n",
    "print(f\"\\nğŸ† êµì°¨ê²€ì¦ ê²°ê³¼ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¨):\")\n",
    "print(f\"  í‰ê·  AUC: {mean_auc:.5f} (Â±{std_auc:.5f})\")\n",
    "print(f\"  OOF AUC: {oof_auc:.5f}\")\n",
    "print(f\"  ìµœê³  Fold AUC: {max(fold_scores):.5f}\")\n",
    "print(f\"  ìµœì € Fold AUC: {min(fold_scores):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2ff770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "  ğŸ“Š ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\n",
      "âœ… í…ŒìŠ¤íŠ¸ íŠ¹ì§• ë¡œë“œ ì™„ë£Œ: (1962, 1664)\n",
      "  Fold 1 ì˜ˆì¸¡ ì™„ë£Œ\n",
      "  Fold 2 ì˜ˆì¸¡ ì™„ë£Œ\n",
      "  Fold 3 ì˜ˆì¸¡ ì™„ë£Œ\n",
      "  Fold 4 ì˜ˆì¸¡ ì™„ë£Œ\n",
      "  Fold 5 ì˜ˆì¸¡ ì™„ë£Œ\n",
      "\n",
      "ğŸ‰ ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ:\n",
      "  íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼: results/submission_hybrid_lgb_20250714_0522.csv\n",
      "  ê¸°ë³¸ íŒŒì¼: submission.csv\n",
      "  ìƒ˜í”Œ ìˆ˜: 1,962ê°œ\n",
      "\n",
      "ğŸ“‹ ìµœì¢… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ:\n",
      "  ğŸ¯ StratifiedKFold í•™ìŠµ: 5-Fold CV ì™„ë£Œ\n",
      "  ğŸ† ìµœì¢… OOF AUC: 0.71047\n",
      "  ğŸ”® ì•™ìƒë¸” ì˜ˆì¸¡: 5ê°œ ëª¨ë¸ í‰ê· \n",
      "  â° ì™„ë£Œ ì‹œê°„: 2025-07-14 05:22:53.046723\n",
      "âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì œì¶œ íŒŒì¼ì„ ëŒ€íšŒì— ì—…ë¡œë“œí•˜ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "\n",
    "# 5ê°œ í´ë“œ ëª¨ë¸ì˜ í‰ê·  ì˜ˆì¸¡\n",
    "print(\"  ğŸ“Š ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\")\n",
    "X_test_features = np.load('data/X_test_paragraph_features.npy')\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ íŠ¹ì§• ë¡œë“œ ì™„ë£Œ: {X_test_features.shape}\")\n",
    "test_predictions_list = []\n",
    "\n",
    "if 'fold_models' not in locals():\n",
    "    print(\"âŒ fold_modelsê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"StratifiedKFold í•™ìŠµ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "for i, model in enumerate(fold_models):\n",
    "    fold_pred = model.predict(X_test_features, num_iteration=model.best_iteration)\n",
    "    test_predictions_list.append(fold_pred)\n",
    "    print(f\"  Fold {i+1} ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "\n",
    "# í‰ê·  ì•™ìƒë¸”\n",
    "ensemble_predictions = np.mean(test_predictions_list, axis=0)\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission['generated'] = ensemble_predictions\n",
    "\n",
    "# ê²°ê³¼ í´ë” ìƒì„± ë° íŒŒì¼ ì €ì¥\n",
    "os.makedirs('results', exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "submission_filename = f'results/submission_hybrid_lgb_{timestamp}.csv'\n",
    "\n",
    "sample_submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "# ê¸°ë³¸ ì œì¶œ íŒŒì¼ë„ ìƒì„±\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nğŸ‰ ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ:\")\n",
    "print(f\"  íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼: {submission_filename}\")\n",
    "print(f\"  ê¸°ë³¸ íŒŒì¼: submission.csv\")\n",
    "print(f\"  ìƒ˜í”Œ ìˆ˜: {len(sample_submission):,}ê°œ\")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\nğŸ“‹ ìµœì¢… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ:\")\n",
    "print(f\"  ğŸ¯ StratifiedKFold í•™ìŠµ: 5-Fold CV ì™„ë£Œ\")\n",
    "print(f\"  ğŸ† ìµœì¢… OOF AUC: {oof_auc:.5f}\")\n",
    "print(f\"  ğŸ”® ì•™ìƒë¸” ì˜ˆì¸¡: 5ê°œ ëª¨ë¸ í‰ê· \")\n",
    "print(f\"  â° ì™„ë£Œ ì‹œê°„: {datetime.now()}\")\n",
    "print(\"âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì œì¶œ íŒŒì¼ì„ ëŒ€íšŒì— ì—…ë¡œë“œí•˜ì„¸ìš”.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
