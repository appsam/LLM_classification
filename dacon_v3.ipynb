{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dd2334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 임포트 완료\n"
     ]
    }
   ],
   "source": [
    "# === 필수 라이브러리 임포트 ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# 머신러닝 모델 및 도구\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 딥러닝 및 임베딩\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0557a89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터를 로드합니다...\n",
      "문단 분리된 데이터프레임을 생성합니다...\n",
      "미리 생성된 특징(.npy) 파일을 로드합니다...\n",
      "훈련 데이터: (1226364, 4)\n",
      "기존 훈련 특징: (1226364, 1664)\n",
      "✅ 데이터 로드 및 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# === 기본 데이터 로드 ===\n",
    "print(\"데이터를 로드합니다...\")\n",
    "train_df_original = pd.read_csv('data/train.csv', encoding='utf-8-sig')\n",
    "test_df = pd.read_csv('data/test.csv', encoding='utf-8-sig')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# === 문단 분리 ===\n",
    "print(\"문단 분리된 데이터프레임을 생성합니다...\")\n",
    "train_df_original['paragraphs'] = train_df_original['full_text'].str.split('\\n')\n",
    "train_paragraph_df = train_df_original.explode('paragraphs').rename(columns={'paragraphs': 'text'})\n",
    "train_paragraph_df.dropna(subset=['text'], inplace=True)\n",
    "train_paragraph_df = train_paragraph_df[train_paragraph_df['text'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "# === 미리 생성된 기본 특징 로드 ===\n",
    "print(\"미리 생성된 특징(.npy) 파일을 로드합니다...\")\n",
    "X_train_paragraph_features = np.load('data/X_train_paragraph_features.npy')\n",
    "X_test_paragraph_features = np.load('data/X_test_paragraph_features.npy')\n",
    "\n",
    "X_train_final = np.load('X_train_final.npy')\n",
    "X_test_final = np.load('X_test_final.npy')\n",
    "\n",
    "print(f\"훈련 데이터: {train_paragraph_df.shape}\")\n",
    "print(f\"기존 훈련 특징: {X_train_paragraph_features.shape}\")\n",
    "print(\"✅ 데이터 로드 및 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aacf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- 1단계 (업그레이드): 문체/통계적 특징 생성 시작 ---\")\n",
    "\n",
    "# # 한국어 불용어 리스트 (필요에 따라 추가/삭제 가능)\n",
    "# STOPWORDS = set(['은', '는', '이', '가', '을', '를', '의', '에', '와', '과', '도', '으로', '로',\n",
    "#                  '하다', '이다', '있다', '그', '저', '이것', '저것', '그것', '및', '등'])\n",
    "# if 'paragraph_text' in test_df.columns:\n",
    "#     print(\"test_df의 'paragraph_text' 컬럼을 'text'로 변경합니다.\")\n",
    "#     test_df.rename(columns={'paragraph_text': 'text'}, inplace=True)\n",
    "\n",
    "# def create_statistical_features(df):\n",
    "#     \"\"\"더 의미있는 통계/문체적 특징을 생성합니다.\"\"\"\n",
    "#     # 기본 개수 특징\n",
    "#     df['text_len'] = df['text'].str.len()\n",
    "#     df['word_count'] = df['text'].str.split().str.len()\n",
    "#     df['sentence_count'] = df['text'].str.count(r'[.!?]') + 1\n",
    "#     df['comma_count'] = df['text'].str.count(',')\n",
    "#     # SyntaxWarning 해결 (r 추가)\n",
    "#     df['special_char_count'] = df['text'].str.count(r'[^A-Za-z0-9가-힣\\s]')\n",
    "#     df['lexical_diversity'] = df['text'].apply(lambda x: len(set(x.split())) / (len(x.split()) + 1e-6))\n",
    "    \n",
    "#     # [새로운 특징] 비율 특징 (0으로 나누는 것을 방지하기 위해 1e-6 추가)\n",
    "#     df['avg_sentence_len'] = df['word_count'] / df['sentence_count']\n",
    "#     df['avg_word_len'] = df['text_len'] / (df['word_count'] + 1e-6)\n",
    "    \n",
    "#     # [새로운 특징] 불용어 비율\n",
    "#     df['stopword_count'] = df['text'].apply(lambda x: sum(word in STOPWORDS for word in x.split()))\n",
    "#     df['stopword_ratio'] = df['stopword_count'] / (df['word_count'] + 1e-6)\n",
    "    \n",
    "#     # 결측치(NaN)가 발생할 경우 0으로 채움\n",
    "#     df.fillna(0, inplace=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # 훈련 및 테스트 데이터에 새로운 함수 적용\n",
    "# train_paragraph_df = create_statistical_features(train_paragraph_df)\n",
    "# test_df = create_statistical_features(test_df)\n",
    "\n",
    "# # 사용할 특징 이름 리스트 업데이트\n",
    "# new_stat_feature_names = [\n",
    "#     'text_len', 'word_count', 'sentence_count', 'comma_count', 'special_char_count', 'lexical_diversity',\n",
    "#     'avg_sentence_len', 'avg_word_len', 'stopword_ratio' # stopword_count는 비율 계산에만 사용\n",
    "# ]\n",
    "# train_stat_features = train_paragraph_df[new_stat_feature_names].values\n",
    "# test_stat_features = test_df[new_stat_feature_names].values\n",
    "\n",
    "# # 기존 특징과 새로 만든 통계 특징을 결합\n",
    "# X_train_combined = np.c_[X_train_paragraph_features, train_stat_features]\n",
    "# X_test_combined = np.c_[X_test_paragraph_features, test_stat_features]\n",
    "\n",
    "# print(\"✅ 업그레이드된 통계 특징 생성 및 결합 완료!\")\n",
    "# print(f\"1단계 후 훈련 특징 shape: {X_train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3aa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- 2단계: KoELECTRA 임베딩 생성 시작 ---\")\n",
    "\n",
    "# # GPU 설정\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# model_name = \"monologg/koelectra-base-v3-discriminator\"\n",
    "# koelectra_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # SafeTensors 사용 및 RTX 3060 최적화\n",
    "# try:\n",
    "#     koelectra_model = AutoModel.from_pretrained(\n",
    "#         model_name,\n",
    "#         use_safetensors=True,  # 보안 강화\n",
    "#         torch_dtype=torch.float16,  # 메모리 효율성 향상\n",
    "#         device_map=\"auto\"  # 자동 메모리 관리\n",
    "#     ).to(device)\n",
    "#     print(\"✅ SafeTensors 형식으로 모델 로드 성공\")\n",
    "# except Exception as e:\n",
    "#     print(f\"⚠️ SafeTensors 로드 실패, 기본 방식으로 시도: {e}\")\n",
    "#     koelectra_model = AutoModel.from_pretrained(model_name).half().to(device)\n",
    "\n",
    "# def get_embeddings(texts, model, tokenizer, batch_size=128):  # RTX 3060 최적화: 256→128\n",
    "#     \"\"\"주어진 모델과 토크나이저로 텍스트 임베딩을 추출합니다.\"\"\"\n",
    "#     all_embeddings = []\n",
    "#     model.eval()\n",
    "    \n",
    "#     for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting Embeddings\"):\n",
    "#         batch = texts[i:i+batch_size]\n",
    "#         batch_dict = tokenizer(\n",
    "#             [str(t) for t in batch], \n",
    "#             max_length=256, \n",
    "#             padding=True, \n",
    "#             truncation=True, \n",
    "#             return_tensors='pt'\n",
    "#         ).to(device)\n",
    "        \n",
    "#         with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#             outputs = model(**batch_dict)\n",
    "        \n",
    "#         embeddings = outputs.last_hidden_state[:, 0, :].float().cpu().numpy()\n",
    "#         all_embeddings.append(embeddings)\n",
    "        \n",
    "#         # 메모리 정리 (10 배치마다)\n",
    "#         if i % 10 == 0:\n",
    "#             torch.cuda.empty_cache()\n",
    "    \n",
    "#     return np.vstack(all_embeddings)\n",
    "\n",
    "# # GPU 메모리 상태 확인\n",
    "# def check_gpu_memory():\n",
    "#     if torch.cuda.is_available():\n",
    "#         print(f\"GPU Memory - Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f}GB, \"\n",
    "#               f\"Reserved: {torch.cuda.memory_reserved(0)/1024**3:.2f}GB\")\n",
    "\n",
    "# print(\"모델 로딩 후 GPU 메모리 상태:\")\n",
    "# check_gpu_memory()\n",
    "\n",
    "# # KoELECTRA 임베딩 생성\n",
    "# print(\"훈련 데이터 임베딩 생성 중...\")\n",
    "# ko_electra_train_features = get_embeddings(train_paragraph_df['text'].tolist(), koelectra_model, koelectra_tokenizer)\n",
    "\n",
    "# print(\"테스트 데이터 임베딩 생성 중...\")\n",
    "# ko_electra_test_features = get_embeddings(test_df['text'].tolist(), koelectra_model, koelectra_tokenizer)\n",
    "\n",
    "# # GPU 메모리 정리\n",
    "# del koelectra_model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"메모리 정리 후 GPU 상태:\")\n",
    "# check_gpu_memory()\n",
    "\n",
    "# # 1단계에서 만든 특징에 KoELECTRA 임베딩을 최종적으로 결합\n",
    "# X_train_final = np.c_[X_train_combined, ko_electra_train_features]\n",
    "# X_test_final = np.c_[X_test_combined, ko_electra_test_features]\n",
    "\n",
    "# # 나중을 위해 최종 특징 파일을 저장해두는 것이 안전합니다.\n",
    "# print(\"최종 특징 파일을 저장합니다...\")\n",
    "# np.save('X_train_final.npy', X_train_final)\n",
    "# np.save('X_test_final.npy', X_test_final)\n",
    "\n",
    "# print(\"✅ KoELECTRA 임베딩 결합 및 최종 특징 생성 완료!\")\n",
    "# print(f\"최종 훈련 특징 shape: {X_train_final.shape}\")\n",
    "# print(f\"최종 테스트 특징 shape: {X_test_final.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2b6a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3단계: 의사 라벨링으로 학습 데이터 정제 시작 ---\n",
      "1차 모델을 Noisy Label로 학습합니다...\n",
      "[LightGBM] [Info] Number of positive: 100712, number of negative: 1125652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 18.613746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 613406\n",
      "[LightGBM] [Info] Number of data points in the train set: 1226364, number of used features: 2441\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.082122 -> initscore=-2.413853\n",
      "[LightGBM] [Info] Start training from score -2.413853\n",
      "모든 훈련 문단에 대해 AI일 확률을 예측합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신뢰도 높은 데이터만 선별하여 최종 훈련 세트를 구성합니다...\n",
      "✅ 데이터 정제 완료!\n",
      "원본 훈련 데이터 수: 1,226,364\n",
      "정제된 훈련 데이터 수: 1,153,741\n",
      "정제된 AI 문단 수: 28,089\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 3단계: 의사 라벨링으로 학습 데이터 정제 시작 ---\")\n",
    "\n",
    "# 1. 1차 모델(LGBM)을 Noisy Label로 빠르게 학습\n",
    "print(\"1차 모델을 Noisy Label로 학습합니다...\")\n",
    "lgb_pseudo = lgb.LGBMClassifier(objective='binary', metric='auc', random_state=42, n_jobs=-1)\n",
    "noisy_labels = train_paragraph_df['generated'].values\n",
    "lgb_pseudo.fit(X_train_final, noisy_labels)\n",
    "\n",
    "# 2. 모든 훈련 데이터에 대해 AI일 확률 예측\n",
    "print(\"모든 훈련 문단에 대해 AI일 확률을 예측합니다...\")\n",
    "train_preds_proba = lgb_pseudo.predict_proba(X_train_final)[:, 1]\n",
    "\n",
    "# 3. 신뢰도 높은 데이터만 선별하여 최종 훈련 세트 구성\n",
    "print(\"신뢰도 높은 데이터만 선별하여 최종 훈련 세트를 구성합니다...\")\n",
    "ai_doc_indices = train_paragraph_df.index[train_paragraph_df['generated'] == 1]\n",
    "human_doc_indices = train_paragraph_df.index[train_paragraph_df['generated'] == 0]\n",
    "\n",
    "# AI 글 중에서, 1차 모델이 AI일 확률이 0.9 이상이라고 매우 강하게 예측한 문단만 선택\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "confident_ai_indices = ai_doc_indices[train_preds_proba[ai_doc_indices] > CONFIDENCE_THRESHOLD]\n",
    "\n",
    "# 최종 훈련에 사용할 인덱스: (확실한 사람 글) + (매우 확실한 AI 글)\n",
    "final_clean_indices = np.concatenate([human_doc_indices, confident_ai_indices])\n",
    "np.random.shuffle(final_clean_indices)\n",
    "\n",
    "# 최종적으로 정제된 훈련 데이터 생성\n",
    "X_train_clean = X_train_final[final_clean_indices]\n",
    "y_train_clean = train_paragraph_df['generated'].iloc[final_clean_indices].values\n",
    "groups_clean = train_paragraph_df['title'].iloc[final_clean_indices].values\n",
    "\n",
    "print(f\"✅ 데이터 정제 완료!\")\n",
    "print(f\"원본 훈련 데이터 수: {len(X_train_final):,}\")\n",
    "print(f\"정제된 훈련 데이터 수: {len(X_train_clean):,}\")\n",
    "print(f\"정제된 AI 문단 수: {(y_train_clean == 1).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24672de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4-1단계: LightGBM 모델 학습 시작 (특징 선택 포함) ---\n",
      "데이터 타입을 float32로 변경하여 메모리 부담을 줄입니다...\n",
      "\n",
      "특징 선택을 시작합니다...\n",
      "200,000개의 샘플로 특징 중요도 측정 모델을 학습합니다...\n",
      "[LightGBM] [Info] Number of positive: 4899, number of negative: 195101\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.764241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 613432\n",
      "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2441\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024495 -> initscore=-3.684486\n",
      "[LightGBM] [Info] Start training from score -3.684486\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "중요도 상위 800개의 특징만 선택하여 새로운 데이터셋을 생성합니다.\n",
      "특징 선택 완료! 새로운 훈련 데이터 shape: (1153741, 800)\n",
      "\n",
      "선택된 특징으로 최종 모델 학습을 시작합니다...\n",
      "--- Fold 1 (LGBM) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 2 (LGBM) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 3 (LGBM) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 4 (LGBM) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 5 (LGBM) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 LightGBM 최종 OOF AUC: 0.99886\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 4-1단계: LightGBM 모델 학습 시작 (특징 선택 포함) ---\")\n",
    "\n",
    "# --- 메모리 절약을 위한 데이터 타입 변경 ---\n",
    "print(\"데이터 타입을 float32로 변경하여 메모리 부담을 줄입니다...\")\n",
    "X_train_clean = X_train_clean.astype('float32')\n",
    "X_test_final = X_test_final.astype('float32')\n",
    "gc.collect()\n",
    "\n",
    "# --- 특징 선택(Feature Selection) 시작 ---\n",
    "print(\"\\n특징 선택을 시작합니다...\")\n",
    "\n",
    "# 1. 중요도 측정을 위해 데이터의 일부 샘플(20만개)만 사용\n",
    "SAMPLE_SIZE = 200000\n",
    "if len(X_train_clean) > SAMPLE_SIZE:\n",
    "    sample_indices = np.random.choice(len(X_train_clean), SAMPLE_SIZE, replace=False)\n",
    "    X_train_sample = X_train_clean[sample_indices]\n",
    "    y_train_sample = y_train_clean[sample_indices]\n",
    "else:\n",
    "    X_train_sample, y_train_sample = X_train_clean, y_train_clean\n",
    "\n",
    "# 2. 샘플 데이터로 단일 LGBM 모델을 빠르게 학습\n",
    "print(f\"{len(X_train_sample):,}개의 샘플로 특징 중요도 측정 모델을 학습합니다...\")\n",
    "fs_model = lgb.LGBMClassifier(objective='binary', metric='auc', n_estimators=500, n_jobs=16, random_state=42)\n",
    "fs_model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# 3. 중요도가 높은 상위 800개 특징의 인덱스를 선택\n",
    "NUM_TOP_FEATURES = 800\n",
    "feature_importances = fs_model.feature_importances_\n",
    "top_feature_indices = np.argsort(feature_importances)[-NUM_TOP_FEATURES:]\n",
    "\n",
    "# 4. 선택된 특징으로 새로운 '슬림' 데이터셋 생성\n",
    "print(f\"중요도 상위 {NUM_TOP_FEATURES}개의 특징만 선택하여 새로운 데이터셋을 생성합니다.\")\n",
    "X_train_slim = X_train_clean[:, top_feature_indices]\n",
    "X_test_slim = X_test_final[:, top_feature_indices]\n",
    "\n",
    "print(f\"특징 선택 완료! 새로운 훈련 데이터 shape: {X_train_slim.shape}\")\n",
    "\n",
    "# 메모리 정리\n",
    "del X_train_sample, y_train_sample, fs_model, feature_importances\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# --- 선택된 특징(X_train_slim)으로 5-Fold 교차검증 진행 ---\n",
    "print(\"\\n선택된 특징으로 최종 모델 학습을 시작합니다...\")\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lgb_oof_preds = np.zeros(len(X_train_slim))\n",
    "lgb_test_preds_list = []\n",
    "lgb_models = []\n",
    "\n",
    "lgb_params = {'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n",
    "              'n_estimators': 2000, 'learning_rate': 0.05, 'num_leaves': 41,\n",
    "              'max_depth': 6, 'seed': 42, 'n_jobs': 16, 'verbose': -1, 'colsample_bytree': 0.8}\n",
    "\n",
    "# groups_clean도 slim 데이터셋에 맞게 인덱싱해야 합니다. (이 부분은 동일)\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_slim, y_train_clean, groups_clean)):\n",
    "    print(f\"--- Fold {fold+1} (LGBM) ---\")\n",
    "    X_train, y_train = X_train_slim[train_idx], y_train_clean[train_idx]\n",
    "    X_val, y_val = X_train_slim[val_idx], y_train_clean[val_idx]\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    \n",
    "    lgb_oof_preds[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    # 테스트 데이터 예측도 slim 버전으로 해야 함\n",
    "    lgb_test_preds_list.append(lgb_model.predict_proba(X_test_slim)[:, 1])\n",
    "    lgb_models.append(lgb_model)\n",
    "    \n",
    "    del X_train, y_train, X_val, y_val, lgb_model\n",
    "    gc.collect()\n",
    "\n",
    "lgb_oof_auc = roc_auc_score(y_train_clean, lgb_oof_preds)\n",
    "print(f\"🏆 LightGBM 최종 OOF AUC: {lgb_oof_auc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40379496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4-2단계: XGBoost 모델 학습 시작 ---\n",
      "--- Fold 1 (XGBoost) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m X_val, y_val = X_train_clean[val_idx], y_train_clean[val_idx]\n\u001b[32m     15\u001b[39m xgb_model = xgb.XGBClassifier(**xgb_params, n_estimators=\u001b[32m2000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m xgb_oof_preds[val_idx] = xgb_model.predict_proba(X_val)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     18\u001b[39m xgb_test_preds_list.append(xgb_model.predict_proba(X_test_final)[:, \u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1660\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1661\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1662\u001b[39m )\n\u001b[32m   1663\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1664\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1665\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1679\u001b[39m     feature_types=\u001b[38;5;28mself\u001b[39m.feature_types,\n\u001b[32m   1680\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1697\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\develop\\deep_learning\\DACON\\venv\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"--- 4-2단계: XGBoost 모델 학습 시작 ---\")\n",
    "\n",
    "xgb_oof_preds = np.zeros(len(X_train_clean))\n",
    "xgb_test_preds_list = []\n",
    "xgb_models = []\n",
    "\n",
    "xgb_params = {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05,\n",
    "              'max_depth': 5, 'subsample': 0.7, 'colsample_bytree': 0.8,\n",
    "              'device': 'cuda', 'tree_method': 'hist', 'random_state': 42}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_clean, y_train_clean, groups_clean)):\n",
    "    print(f\"--- Fold {fold+1} (XGBoost) ---\")\n",
    "    X_train, y_train = X_train_clean[train_idx], y_train_clean[train_idx]\n",
    "    X_val, y_val = X_train_clean[val_idx], y_train_clean[val_idx]\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params, n_estimators=2000)\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
    "    xgb_oof_preds[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    xgb_test_preds_list.append(xgb_model.predict_proba(X_test_final)[:, 1])\n",
    "    xgb_models.append(xgb_model)\n",
    "\n",
    "xgb_oof_auc = roc_auc_score(y_train_clean, xgb_oof_preds)\n",
    "print(f\"🏆 XGBoost 최종 OOF AUC: {xgb_oof_auc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da5bde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5단계: 최종 제출 (LGBM 단일 모델) ---\n",
      "LightGBM 단일 모델의 예측값으로 제출 파일을 생성합니다.\n",
      "🎉 최종 제출 파일 생성 완료: submission_lgbm_only_20250715_0916.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 5단계: 최종 제출 (LGBM 단일 모델) ---\")\n",
    "\n",
    "# LightGBM 5-Fold 모델들의 테스트 데이터 예측값 평균 계산\n",
    "lgb_final_preds = np.mean(lgb_test_preds_list, axis=0)\n",
    "print(\"LightGBM 단일 모델의 예측값으로 제출 파일을 생성합니다.\")\n",
    "\n",
    "# 제출 파일 생성\n",
    "sample_submission['generated'] = lgb_final_preds\n",
    "\n",
    "# 파일 이름에 lgbm_only를 추가하여 구분\n",
    "submission_filename = f\"submission_lgbm_only_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "sample_submission.to_csv(submission_filename, index=False)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False) # 기본 파일도 생성\n",
    "\n",
    "print(f\"🎉 최종 제출 파일 생성 완료: {submission_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
